{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M3 Week 2 Group 8: Daniel Azemar, Mar√≠a Gil, Richard Segovia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report structure\n",
    "First you can find the main code if you want to look the results scroll down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "\n",
    "from assessment import showConfusionMatrix\n",
    "from descriptors import get_bag_of_words, get_visual_words, compute_descriptors, get_pyramid_visual_word_len, select_descriptors\n",
    "from classifiers import get_dist_func, select_svm_kernel\n",
    "from main import save_data, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "\n",
    "from assessment import showConfusionMatrix\n",
    "from descriptors import get_bag_of_words, get_visual_words, compute_descriptors, get_pyramid_visual_word_len, select_descriptors\n",
    "from classifiers import get_dist_func, select_svm_kernel\n",
    "\n",
    "class Classification(object):\n",
    "    def __init__(self):\n",
    "        # Load train and test files\n",
    "        self.total_train_images_filenames = pickle.load(open('./train_images_filenames.dat', 'rb'))\n",
    "        self.total_test_images_filenames = pickle.load(open('./test_images_filenames.dat', 'rb'))\n",
    "        self.total_train_labels = pickle.load(open('./train_labels.dat', 'rb'))\n",
    "        self.total_test_labels = pickle.load(open('./test_labels.dat', 'rb'))\n",
    "\n",
    "        #Cross-validation parameters\n",
    "        self.split_num = 2 #number of k-folds\n",
    "\n",
    "        # Descriptors parameters\n",
    "        self.kp_detector = 'dense' # sift dense\n",
    "        self.desc_type = 'sift' # sift\n",
    "        self.n_descriptors = 600\n",
    "\n",
    "        # Dense SIFT parameters\n",
    "        self.stepValue = 10\n",
    "        self.scale_mode = \"gauss\"  # multiple random, uniform, gauss\n",
    "        # uniform scale params\n",
    "        self.maxScale = 15\n",
    "        self.minScale = 7\n",
    "        # gauss scale params\n",
    "        self.mean = 15\n",
    "        self.desvt = 7\n",
    "\n",
    "        #BagOfVisualWords parameters\n",
    "        self.mode_bagofWords = 'pyramids' # all pyramids\n",
    "        self.reduce_num_of_features = True\n",
    "        self.features_per_img = 100\n",
    "\n",
    "        #Spatial pyramid params\n",
    "        self.levels_pyramid = 2\n",
    "        self.codebook_size = 128\n",
    "        self.normalize_level_vw = True\n",
    "        self.scaleData_level_vw = False\n",
    "\n",
    "        #Data normalization and scalation\n",
    "        self.normalize = True\n",
    "        self.scaleData = False\n",
    "\n",
    "        #Classifier parameters\n",
    "        self.classif_type  =  'svm' # knn svm\n",
    "        self.knn_metric = 'euclidean'\n",
    "        self.svm_metric = 'hist_intersection' #'rbf' or 'hist_intersection'\n",
    "        self.save_trainData = False\n",
    "        #SVM parameters\n",
    "        self.C=1.0\n",
    "        self.degree=3\n",
    "        self.gamma='auto'\n",
    "\n",
    "\n",
    "    def compute(self):\n",
    "        # Split train dataset for cross-validation\n",
    "        cv = StratifiedKFold(n_splits=self.split_num)\n",
    "\n",
    "        accumulated_accuracy=[]        \n",
    "        for train_index, val_index in cv.split(self.total_train_images_filenames, self.total_train_labels):\n",
    "            train_index = train_index[:200]\n",
    "            val_index = val_index[:200]\n",
    "            train_filenames = [self.total_train_images_filenames[index] for index in train_index]\n",
    "            train_labels = [self.total_train_labels[index] for index in train_index]\n",
    "            val_filenames = [self.total_train_images_filenames[index] for index in val_index]\n",
    "            val_labels = [self.total_train_labels[index] for index in val_index]\n",
    "\n",
    "            # TRAIN CLASSIFIER\n",
    "            keypoint_list = []\n",
    "            train_desc_list = []\n",
    "            train_label_per_descriptor = []\n",
    "\n",
    "            for filename, labels in zip(tqdm(train_filenames, desc=\"train descriptors\"), train_labels):\n",
    "                ima = cv2.imread(filename)\n",
    "                kpt, des = compute_descriptors(ima, self.kp_detector, self.desc_type, self.stepValue, self.scale_mode, self.minScale, self.maxScale, self.mean, self.desvt, self.n_descriptors)\n",
    "                keypoint_list.append(kpt)\n",
    "                train_desc_list.append(des)\n",
    "                train_label_per_descriptor.append(labels)\n",
    "            \n",
    "            D = np.vstack(train_desc_list)\n",
    "\n",
    "            # reducing the number of descriptors used in bag of words\n",
    "            if(self.reduce_num_of_features):\n",
    "                selected_index = select_descriptors(train_desc_list, self.features_per_img)\n",
    "                D = D[selected_index]\n",
    "\n",
    "            # 3. Create codebook and fit with train dataset\n",
    "            codebook, visual_words = get_bag_of_words(self.levels_pyramid, self.mode_bagofWords, D, train_desc_list, keypoint_list, self.codebook_size, normalize_level_vw=self.normalize_level_vw, scaleData_level_vw=self.scaleData_level_vw)\n",
    "\n",
    "            # 4. self.normalize and scale descriptors\n",
    "            if(self.normalize):\n",
    "                norm_model = Normalizer().fit(visual_words) #l2 norm by default\n",
    "                visual_words = norm_model.fit_transform(visual_words)\n",
    "\n",
    "            #scaling\n",
    "            if(self.scaleData):\n",
    "                scale_model = StandardScaler().fit(visual_words)\n",
    "                visual_words = scale_model.fit_transform(visual_words)\n",
    "\n",
    "\n",
    "            # 5. Train classifier\n",
    "            model = None\n",
    "            if self.classif_type == 'knn':\n",
    "                model = KNeighborsClassifier(n_neighbors=5, n_jobs=-1, metric=get_dist_func(self.knn_metric))\n",
    "                model.fit(visual_words, train_labels)\n",
    "            elif self.classif_type == 'svm':\n",
    "                svm_kernel = select_svm_kernel(self.svm_metric)\n",
    "                model = SVC(C=self.C, kernel=svm_kernel, degree=self.degree, gamma=self.gamma, shrinking=False, probability=False, tol=0.001, max_iter=-1)\n",
    "                model.fit(visual_words, train_labels)\n",
    "            else:\n",
    "                raise (NotImplemented(\"self.classif_type not implemented or not recognized:\" + str(self.classif_type)))\n",
    "\n",
    "            # 6. Save/load data in pickle\n",
    "            if self.save_trainData:\n",
    "                save_data(codebook, \"codebook.pkl\")\n",
    "                save_data(visual_words, \"visual_words.pkl\")\n",
    "            else:\n",
    "                pass\n",
    "                #codebook = load_data(\"codebook.pkl\")\n",
    "                #visual_words = load_data(\"visual_words.pkl\")\n",
    "\n",
    "            # VALIDATE CLASSIFIER WITH CROSS-VALIDATION DATASET\n",
    "\n",
    "            if(self.mode_bagofWords == 'all'):\n",
    "                visual_words_test = np.zeros((len(val_filenames), self.codebook_size), dtype=np.float32)\n",
    "            if(self.mode_bagofWords == 'pyramids'):\n",
    "                len_vw = get_pyramid_visual_word_len(self.levels_pyramid,self.codebook_size)\n",
    "                visual_words_test = np.zeros((len(val_filenames), len_vw), dtype=np.float32)\n",
    "\n",
    "            for i in tqdm(range(len(val_filenames)), desc=\"test descriptors\"):\n",
    "                filename = val_filenames[i]\n",
    "                ima = cv2.imread(filename)\n",
    "                kpt, des = compute_descriptors(ima, self.kp_detector, self.desc_type, self.stepValue, self.scale_mode, self.minScale, self.maxScale, self.mean, self.desvt, self.n_descriptors )\n",
    "\n",
    "                _, visual_words_test[i,:] = get_visual_words(self.levels_pyramid, self.mode_bagofWords, codebook, des, kpt, self.codebook_size,  normalize_level_vw=self.normalize_level_vw, scaleData_level_vw=self.scaleData_level_vw)\n",
    "\n",
    "            if(self.normalize):\n",
    "                visual_words_test = norm_model.transform(visual_words_test)\n",
    "            if(self.scaleData):\n",
    "                visual_words_test = scale_model.transform(visual_words_test)\n",
    "\n",
    "            # ASSESSMENT OF THE CLASSIFIER\n",
    "            accuracy = 100 * model.score(visual_words_test, val_labels)\n",
    "            accumulated_accuracy.append(accuracy)\n",
    "\n",
    "            # Show Confusion Matrix\n",
    "            # showConfusionMatrix(dist_name_list, conf_mat_list, labels_names)\n",
    "\n",
    "            return np.sum(accumulated_accuracy)/len(accumulated_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "Along the experiments implemented in this report, we used the cross-validation method: StratifiedKFolds, from the scikit-learn library. The number of subsets was set to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize class Classification() with default parameters\n",
    "classifier_images = Classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptor extraction improvements\n",
    "#### Use Dense SIFT even more dense\n",
    "Use Dense SIFT with a step size of 15 and 4 different scales from [4, 8, 12, 16]. \n",
    "Select a random selection of a subset of keypoints for clustering when generating the codebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train descriptors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:14<00:00, 13.84it/s]\n",
      "test descriptors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:15<00:00, 12.74it/s]\n",
      "train descriptors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:15<00:00, 12.91it/s]\n",
      "test descriptors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:15<00:00, 13.32it/s]\n",
      "train descriptors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:16<00:00, 11.16it/s]\n",
      "test descriptors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:15<00:00, 13.11it/s]\n",
      "train descriptors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:14<00:00, 13.78it/s]\n",
      "test descriptors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:15<00:00, 13.43it/s]\n"
     ]
    }
   ],
   "source": [
    "#Set experiment parameters\n",
    "classifier_images.reduce_num_of_features = True\n",
    "classifier_images.stepValue = 15\n",
    "subset_sizes = [100, 250, 500, 750]\n",
    "results_subset_sizes = {}\n",
    "#Run experiment\n",
    "for subset_size in subset_sizes:\n",
    "    classifier_images.features_per_img = subset_size\n",
    "    results_subset_sizes[str(subset_size)] = classifier_images.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'100': 84.0, '250': 83.5, '500': 83.0, '750': 85.0}\n"
     ]
    }
   ],
   "source": [
    "#TODO print grafica \n",
    "print(results_subset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO escribir analisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial Pyramids\n",
    "Even with Dense SIFT descriptors, there is a lack of spatial information that can be very useful at classification. In order to preserve this information, we introduced the Spatial Pyramid implementation at the codebook and bag of visual words step. Data is normalized before being stacked, otherwise the resulting descriptors would not have the same relevance depending on their scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pyramid levels selection\n",
    "classifier_images.mode_bagofWords = \"pyramid\"\n",
    "levels = [2,3,4]\n",
    "results_pyramid_levels = {}\n",
    "#Run experiment\n",
    "for level in levels:\n",
    "    classifier_images.levels_pyramid = level\n",
    "    results_subset_sizes[str(subset_size)] = classifier_images.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze improvement introduced by spatial pyramids\n",
    "classifier_images.mode_bagofWords = \"pyramid\"\n",
    "classifier_images.levels_pyramid = 2 #best result in previous step\n",
    "print(classifier_images.compute())\n",
    "\n",
    "classifier_images.mode_bagofWords = \"all\"\n",
    "print(classifier_images.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the SVM classifier\n",
    "#### Parameter selection: cost and gamma\n",
    "Intuitively, the gamma parameter defines how far the influence of a single training example reaches, with low values meaning ‚Äòfar‚Äô and high values meaning ‚Äòclose‚Äô. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.\n",
    "\n",
    "The C parameter trades off correct classification of training examples against maximization of the decision function‚Äôs margin. For larger values of C, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. A lower C will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy. In other words``C`` behaves as a regularization parameter in the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set experiment parameters\n",
    "cost = [2,4,8,16]\n",
    "gamma = [1,2,4,8,16,32]\n",
    "results_svm_params = {}\n",
    "#Run experiment\n",
    "for c in cost:\n",
    "    classifier_images.C = c\n",
    "    for g in gamma:\n",
    "        classifier_images.gamma = g\n",
    "        results_svm_kernels[str([c,g])] = classifier_images.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel selection: linear, RBF, histogram_intersection\n",
    "The scikit-learn library provides some tools to find the decision boundary, as the linear or RBF kernel. However, in the case of our data, since we aim to compare histograms, a histogram intersection kernel tool has also been implemented. During this experiment we run a comparison between the three different kernels and provide some analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set experiment parameters\n",
    "kernels = ['linear', 'RBF', 'hist_intersection']\n",
    "results_svm_kernels = {}\n",
    "#Run experiment\n",
    "for kernel in kernels:\n",
    "    classifier_images.svm_metric = kernel\n",
    "    results_svm_kernels[str(subset_size)] = classifier_images.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codebook size selection\n",
    "When using SVM instead of the KNN classifier, the codebook size plays an important role as well. In the following experiment we iterated over different values for the codebook size and selected the one with the best cross-validated score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train descriptors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:33<00:00,  5.14it/s]\n",
      "test descriptors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:37<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train descriptors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:31<00:00,  6.70it/s]\n",
      "test descriptors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:32<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.0\n"
     ]
    }
   ],
   "source": [
    "codebook_sizes = [128, 256]\n",
    "\n",
    "for codebook_size in codebook_sizes:\n",
    "    classifier_images.codebook_size = codebook_size\n",
    "    print(classifier_images.compute())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of normalization and scaling\n",
    "Effects of normalizing and/or scaling data before training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run experiment with and without data normalization\n",
    "classifier_images.normalize = True\n",
    "print(classifier_images.compute())\n",
    "classifier_images.normalize = False\n",
    "print(classifier_images.compute())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
