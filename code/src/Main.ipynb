{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M3 Week 2 Group 8: Daniel Azemar, María Gil, Richard Segovia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report structure\n",
    "First you can find the main code if you want to look the results scroll down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "\n",
    "from assessment import showConfusionMatrix\n",
    "from descriptors import get_bag_of_words, get_visual_words, compute_descriptors, get_pyramid_visual_word_len, select_descriptors\n",
    "from classifiers import get_dist_func, select_svm_kernel\n",
    "from main import save_data, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "\n",
    "from assessment import showConfusionMatrix\n",
    "from descriptors import get_bag_of_words, get_visual_words, compute_descriptors, get_pyramid_visual_word_len, select_descriptors\n",
    "from classifiers import get_dist_func, select_svm_kernel\n",
    "\n",
    "class Classification(object):\n",
    "    def __init__(self):\n",
    "        # Load train and test files\n",
    "        self.total_train_images_filenames = pickle.load(open('./train_images_filenames.dat', 'rb'))\n",
    "        self.total_test_images_filenames = pickle.load(open('./test_images_filenames.dat', 'rb'))\n",
    "        self.total_train_labels = pickle.load(open('./train_labels.dat', 'rb'))\n",
    "        self.total_test_labels = pickle.load(open('./test_labels.dat', 'rb'))\n",
    "\n",
    "        #Cross-validation parameters\n",
    "        self.split_num = 2 #number of k-folds\n",
    "\n",
    "        # Descriptors parameters\n",
    "        self.kp_detector = 'dense' # sift dense\n",
    "        self.desc_type = 'sift' # sift\n",
    "        self.n_descriptors = 600\n",
    "\n",
    "        # Dense SIFT parameters\n",
    "        self.stepValue = 10\n",
    "        self.scale_mode = \"gauss\"  # multiple random, uniform, gauss\n",
    "        # uniform scale params\n",
    "        self.maxScale = 15\n",
    "        self.minScale = 7\n",
    "        # gauss scale params\n",
    "        self.mean = 15\n",
    "        self.desvt = 7\n",
    "\n",
    "        #BagOfVisualWords parameters\n",
    "        self.mode_bagofWords = 'pyramids' # all pyramids\n",
    "        self.reduce_num_of_features = True\n",
    "        self.features_per_img = 100\n",
    "\n",
    "        #Spatial pyramid params\n",
    "        self.levels_pyramid = 2\n",
    "        self.codebook_size = 128\n",
    "        self.normalize_level_vw = True\n",
    "        self.scaleData_level_vw = False\n",
    "\n",
    "        #Data normalization and scalation\n",
    "        self.normalize = True\n",
    "        self.scaleData = False\n",
    "\n",
    "        #Classifier parameters\n",
    "        self.classif_type  =  'svm' # knn svm\n",
    "        self.knn_metric = 'euclidean'\n",
    "        self.svm_metric = 'hist_intersection' #'rbf' or 'hist_intersection'\n",
    "        self.save_trainData = False\n",
    "        #SVM parameters\n",
    "        self.C=1.0\n",
    "        self.degree=3\n",
    "        self.gamma='auto'\n",
    "\n",
    "\n",
    "    def compute(self):\n",
    "        # Split train dataset for cross-validation\n",
    "        cv = StratifiedKFold(n_splits=self.split_num)\n",
    "\n",
    "        accumulated_accuracy=[]        \n",
    "        for train_index, val_index in cv.split(self.total_train_images_filenames, self.total_train_labels):\n",
    "            train_index = train_index[:200]\n",
    "            val_index = val_index[:200]\n",
    "            train_filenames = [self.total_train_images_filenames[index] for index in train_index]\n",
    "            train_labels = [self.total_train_labels[index] for index in train_index]\n",
    "            val_filenames = [self.total_train_images_filenames[index] for index in val_index]\n",
    "            val_labels = [self.total_train_labels[index] for index in val_index]\n",
    "\n",
    "            # TRAIN CLASSIFIER\n",
    "            keypoint_list = []\n",
    "            train_desc_list = []\n",
    "            train_label_per_descriptor = []\n",
    "\n",
    "            for filename, labels in zip(tqdm(train_filenames, desc=\"train descriptors\"), train_labels):\n",
    "                ima = cv2.imread(filename)\n",
    "                kpt, des = compute_descriptors(ima, self.kp_detector, self.desc_type, self.stepValue, self.scale_mode, self.minScale, self.maxScale, self.mean, self.desvt, self.n_descriptors)\n",
    "                keypoint_list.append(kpt)\n",
    "                train_desc_list.append(des)\n",
    "                train_label_per_descriptor.append(labels)\n",
    "            \n",
    "            D = np.vstack(train_desc_list)\n",
    "\n",
    "            # reducing the number of descriptors used in bag of words\n",
    "            if(self.reduce_num_of_features):\n",
    "                selected_index = select_descriptors(train_desc_list, self.features_per_img)\n",
    "                D = D[selected_index]\n",
    "\n",
    "            # 3. Create codebook and fit with train dataset\n",
    "            codebook, visual_words = get_bag_of_words(self.levels_pyramid, self.mode_bagofWords, D, train_desc_list, keypoint_list, self.codebook_size, normalize_level_vw=self.normalize_level_vw, scaleData_level_vw=self.scaleData_level_vw)\n",
    "\n",
    "            # 4. self.normalize and scale descriptors\n",
    "            if(self.normalize):\n",
    "                norm_model = Normalizer().fit(visual_words) #l2 norm by default\n",
    "                visual_words = norm_model.fit_transform(visual_words)\n",
    "\n",
    "            #scaling\n",
    "            if(self.scaleData):\n",
    "                scale_model = StandardScaler().fit(visual_words)\n",
    "                visual_words = scale_model.fit_transform(visual_words)\n",
    "\n",
    "\n",
    "            # 5. Train classifier\n",
    "            model = None\n",
    "            if self.classif_type == 'knn':\n",
    "                model = KNeighborsClassifier(n_neighbors=5, n_jobs=-1, metric=get_dist_func(self.knn_metric))\n",
    "                model.fit(visual_words, train_labels)\n",
    "            elif self.classif_type == 'svm':\n",
    "                svm_kernel = select_svm_kernel(self.svm_metric)\n",
    "                model = SVC(C=self.C, kernel=svm_kernel, degree=self.degree, gamma=self.gamma, shrinking=False, probability=False, tol=0.001, max_iter=-1)\n",
    "                model.fit(visual_words, train_labels)\n",
    "            else:\n",
    "                raise (NotImplemented(\"self.classif_type not implemented or not recognized:\" + str(self.classif_type)))\n",
    "\n",
    "            # 6. Save/load data in pickle\n",
    "            if self.save_trainData:\n",
    "                save_data(codebook, \"codebook.pkl\")\n",
    "                save_data(visual_words, \"visual_words.pkl\")\n",
    "            else:\n",
    "                pass\n",
    "                #codebook = load_data(\"codebook.pkl\")\n",
    "                #visual_words = load_data(\"visual_words.pkl\")\n",
    "\n",
    "            # VALIDATE CLASSIFIER WITH CROSS-VALIDATION DATASET\n",
    "\n",
    "            if(self.mode_bagofWords == 'all'):\n",
    "                visual_words_test = np.zeros((len(val_filenames), self.codebook_size), dtype=np.float32)\n",
    "            if(self.mode_bagofWords == 'pyramids'):\n",
    "                len_vw = get_pyramid_visual_word_len(self.levels_pyramid,self.codebook_size)\n",
    "                visual_words_test = np.zeros((len(val_filenames), len_vw), dtype=np.float32)\n",
    "\n",
    "            for i in tqdm(range(len(val_filenames)), desc=\"test descriptors\"):\n",
    "                filename = val_filenames[i]\n",
    "                ima = cv2.imread(filename)\n",
    "                kpt, des = compute_descriptors(ima, self.kp_detector, self.desc_type, self.stepValue, self.scale_mode, self.minScale, self.maxScale, self.mean, self.desvt, self.n_descriptors )\n",
    "\n",
    "                _, visual_words_test[i,:] = get_visual_words(self.levels_pyramid, self.mode_bagofWords, codebook, des, kpt, self.codebook_size,  normalize_level_vw=self.normalize_level_vw, scaleData_level_vw=self.scaleData_level_vw)\n",
    "\n",
    "            if(self.normalize):\n",
    "                visual_words_test = norm_model.transform(visual_words_test)\n",
    "            if(self.scaleData):\n",
    "                visual_words_test = scale_model.transform(visual_words_test)\n",
    "\n",
    "            # ASSESSMENT OF THE CLASSIFIER\n",
    "            accuracy = 100 * model.score(visual_words_test, val_labels)\n",
    "            accumulated_accuracy.append(accuracy)\n",
    "\n",
    "            # Show Confusion Matrix\n",
    "            # showConfusionMatrix(dist_name_list, conf_mat_list, labels_names)\n",
    "\n",
    "            return np.sum(accumulated_accuracy)/len(accumulated_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_images = Classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train descriptors: 100%|██████████| 200/200 [00:33<00:00,  5.14it/s]\n",
      "test descriptors: 100%|██████████| 200/200 [00:37<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train descriptors:  66%|██████▌   | 132/200 [00:21<00:09,  6.89it/s]"
     ]
    }
   ],
   "source": [
    "codebook_sizes = [128, 256]\n",
    "\n",
    "for codebook_s in codebook_sizes:\n",
    "    classifier_images.codebook_size = codebook_s\n",
    "    print(classifier_images.compute())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
